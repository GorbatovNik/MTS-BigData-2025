## Автоматический запуск и применение Apache Spark

Скрипт запускает Apache Spark, копирует данные в HDFS и создает репартиционированную таблицу в HIVE, совершает манипуляции с датафреймом для демонстрации.

### Основные шаги
1. **Запуск Hive Metastore**
   - На узле `$NN` запускается сервис `hive --service metastore` в фоне, логи сохраняются в `/tmp/metastore.log`.

2. **Создание тестовой базы данных**
   - Через Beeline создаёт или пересоздаёт базу данных `test` в Hive.


3. **Загрузка тестовых данных**
   - Скачивает тестовый CSV-файл через `$DATA_URL`.
   - Передаёт файл на `$NN` и загружает его в HDFS в `/input`.

4. **Копирование окружения Hadoop и Hive на `$JN`**
   - Создаёт пользователя `$HADOOP_USER` на `$JN`.
   - Копирует с `$NN` директории Hadoop (`hadoop-3.4.0`), Hive (`apache-hive-4.0.0-alpha-2-bin`) и `.profile`.

5. **Настройка Python и окружения Spark**
   - Устанавливает `python3-venv`, `python3-pip`.
   - Создаёт виртуальное окружение `.venv` от имени `$HADOOP_USER`.
   - Устанавливает библиотеки:
     - `pyspark==3.5.6`
     - `onetl`
   - Запускает Python-скрипт `spark_setup.py`.

6. **Выполнение `spark_setup.py`**
   - Создаёт `SparkSession` с поддержкой Hive и YARN.  
   - Проверяет соединения с HDFS и Hive.  
   - Считывает CSV-файл из `/input` в DataFrame.  
   - Записывает данные в Hive таблицу `test.<data_name>` и репартиционированную версию `test.<data_name>_1`.  
   - Выполняет тестовые операции над DataFrame.

### Требования

- Развернутый (c помощью скрипта из hw1) HDFS на кластере.
- Запущенный (c помощью скрипта из hw2) YARN на кластере.
- Запущенный (c помощью скрипта из hw3) HIVE на кластере.
- Доступные файлы рядом со скриптом:  
  - `spark_setup.py`

### Запуск
```bash
bash preprocess_configs.sh
bash setup_spark.sh
```

### Запуск веб-интерфейсов с локальной машины

```bash
ssh -L 10002:{внутренний адрес namenode}:10002 9870:{внутренний адрес namenode}:9870 -L 8088:{внутренний адрес namenode}:8088 -L 19888:{внутренний адрес namenode}:19888 team@{внешний адрес для входа}
```